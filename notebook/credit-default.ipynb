{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**End to end Machine learning project from training to deployment**\n",
        "\n",
        "We start by importing the necessary packages and authenticating with Azure ML. DefaultAzureCredential automatically handles authentication in different environments, such as Azure notebooks or local machines with Azure CLI logged in"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913434475
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "\n",
        "If DefaultAzureCredential does not work, we can use InteractiveBrowserCredential to manually authenticate via a browser popup."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "# from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "# from azure.identity import InteractiveBrowserCredential\n",
        "# credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to Azure ML workspace**\n",
        "\n",
        "Explanation:\n",
        "We create an MLClient instance to interact with our workspace. This client allows us to register datasets, models, environments, and submit pipeline jobs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"cb51c13f-04b1-4395-8005-e4f9f2b5e397\",\n",
        "    resource_group_name=\"mlresources\",\n",
        "    workspace_name=\"MLAssessment\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913443088
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create dependencies directory**\n",
        "\n",
        "Explanation:\n",
        "We create a folder to store environment dependency files like conda.yaml."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"../dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756901959890
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define environment dependencies**\n",
        "\n",
        "Explanation:\n",
        "We define a conda.yaml file listing Python packages and pip dependencies required for training and inference."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow== 1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - psutil>=5.8,<5.9\n",
        "    - tqdm>=4.59,<4.60\n",
        "    - ipykernel~=6.0\n",
        "    - matplotlib"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"aml-scikit-learnv2\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756902282278
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare source code folder**\n",
        "\n",
        "Explanation:\n",
        "We create a folder for our training scripts."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_src_dir = \"../src\"\n",
        "os.makedirs(train_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756902731343
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write main training script**\n",
        "\n",
        "**Explanation:**\n",
        "The training script main.py handles:\n",
        "\n",
        "- Loading input data\n",
        "\n",
        "- Preprocessing\n",
        "\n",
        "- Training a GradientBoostingClassifier\n",
        "\n",
        "- Logging metrics with MLflow\n",
        "\n",
        "- Saving and registering the trained model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/main.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    # enable autologging\n",
        "    mlflow.sklearn.autolog()\n",
        "\n",
        "    ###################\n",
        "    #<prepare the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "    \n",
        "    credit_df = pd.read_excel(args.data, header=1, index_col=0)\n",
        "\n",
        "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
        "\n",
        "    train_df, test_df = train_test_split(\n",
        "        credit_df,\n",
        "        test_size=args.test_train_ratio,\n",
        "    )\n",
        "    ####################\n",
        "    #</prepare the data>\n",
        "    ####################\n",
        "\n",
        "    ##################\n",
        "    #<train the model>\n",
        "    ##################\n",
        "    # Extracting the label column\n",
        "    y_train = train_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_df.values\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_df.values\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    clf = GradientBoostingClassifier(\n",
        "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    ###################\n",
        "    #</train the model>\n",
        "    ###################\n",
        "\n",
        "    ##########################\n",
        "    #<save and register model>\n",
        "    ##########################\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=clf,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=clf,\n",
        "        path=os.path.join(args.registered_model_name, \"trained_model\"),\n",
        "    )\n",
        "    ###########################\n",
        "    #</save and register model>\n",
        "    ###########################\n",
        "    \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get data asset**\n",
        "\n",
        "Explanation:\n",
        "We retrieve the dataset registered in Azure ML to use for training."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data asset\n",
        "data_asset = ml_client.data.get(name=\"default_credit_card_dataset\", version=\"1\")\n",
        "print(data_asset.path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913456710
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define training step**\n",
        "\n",
        "Explanation:\n",
        "We define a command step in Azure ML that runs our training script. This step is later used in a pipeli"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "registered_model_name = \"credit_defaults_model\"\n",
        "\n",
        "train_step = command(\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=\"uri_file\",\n",
        "            path=data_asset.path,\n",
        "        ),\n",
        "        test_train_ratio=0.2,\n",
        "        learning_rate=0.25,\n",
        "        registered_model_name=registered_model_name,\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        model_output=Output(type=\"uri_folder\")  # <-- define output\n",
        "    ),\n",
        "    code=\"../src/\",  # location of source code\n",
        "    command=\"python main2.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}} --model_output ${{outputs.model_output}}\",\n",
        "    environment=\"aml-scikit-learnv2@latest\",\n",
        "    experiment_name=\"train_model_credit_default_prediction\",\n",
        "    display_name=\"credit_default_prediction\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913468156
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ml_client.create_or_update(job)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756903903719
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define pipeline**\n",
        "\n",
        "Explanation:\n",
        "We create a pipeline that runs the training step. Pipelines allow us to chain multiple steps, e.g., training → evaluation → deployment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command, Input, dsl\n",
        "from azure.ai.ml.entities import PipelineJob\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"cpu-cluster1\",  # replace with your cluster name\n",
        "    description=\"Pipeline for credit default prediction\",\n",
        ")\n",
        "def credit_default_pipeline():\n",
        "    train_job = train_step()\n",
        "    # later you can add more steps like:\n",
        "    # evaluate_job = evaluate_component(inputs=...)\n",
        "    # evaluate_job.run_after(train_job)\n",
        "    return {\"model_output\": train_job.outputs.model_output}\n",
        "\n",
        "# Create pipeline job instance\n",
        "pipeline_job: PipelineJob = credit_default_pipeline()\n",
        "\n",
        "# Submit pipeline job\n",
        "submitted_job = ml_client.jobs.create_or_update(pipeline_job)\n",
        "print(f\"Pipeline job submitted: {submitted_job.name}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913522314
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create online endpoint**\n",
        "\n",
        "Explanation:\n",
        "We create a managed online endpoint for real-time model inference. We also generate a unique name to avoid naming conflicts."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Creating a unique name for the endpoint\n",
        "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913528142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\n",
        "        \"training_dataset\": \"credit_defaults\",\n",
        "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
        "    },\n",
        ")\n",
        "\n",
        "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "\n",
        "print(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913630082
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieve endpoint**\n",
        "\n",
        "Explanation:\n",
        "Retrieve the online endpoint to verify its provisioning state and confirm it’s ready for deployments."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "print(\n",
        "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913637740
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploy model**\n",
        "\n",
        "Explanation:\n",
        "We pick the latest registered model version and deploy it to the managed endpoint."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pick the latest version of the model\n",
        "latest_model_version = max(\n",
        "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
        ")\n",
        "\n",
        "print(latest_model_version)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756913641626
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# picking the model to deploy. Here we use the latest version of our registered model\n",
        "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "\n",
        "# create an online deployment.\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    environment=\"aml-scikit-learnv2@latest\",\n",
        "    instance_type=\"Standard_DS3_v2\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "blue_deployment = ml_client.begin_create_or_update(blue_deployment).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756917074007
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}